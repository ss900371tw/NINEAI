# app_transparency.py
import os
import re
import tempfile
import streamlit as st
import fitz  # PyMuPDF
from dotenv import load_dotenv
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from difflib import SequenceMatcher
from PIL import Image
import ollama  # âœ… æ”¹ç‚ºä½¿ç”¨ Ollama
import google.generativeai as genai
from dotenv import load_dotenv
import os

# ---------- åˆå§‹åŒ– ----------
# âœ… åˆå§‹åŒ– Gemini
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
genai.configure(api_key=GOOGLE_API_KEY)

# ä½¿ç”¨ Gemini 2.5 Pro æ¨¡å‹
model = genai.GenerativeModel("gemini-2.5-pro")


# FAISS å‘é‡åº«åˆå§‹åŒ–ï¼ˆè«‹ç¢ºä¿ INDEX_FILE_PATH èˆ‡ embeddings è¨­å®šæ­£ç¢ºï¼‰
INDEX_FILE_PATH = "faiss_index"
vector_store = None
try:
    vector_store = FAISS.load_local(INDEX_FILE_PATH, embeddings=HuggingFaceEmbeddings(), allow_dangerous_deserialization=True)
except Exception as e:
    vector_store = None
    print("âš ï¸ ç„¡æ³•è¼‰å…¥ FAISS å‘é‡åº«ï¼š", e)

# ---------- ä¹å¤§é€æ˜æ€§åŸå‰‡å®šç¾© ----------
TRANSPARENCY_PRINCIPLES = [
    "ä»‹å…¥è©³æƒ…åŠè¼¸å‡ºï¼šèªªæ˜äººå·¥æ™ºæ…§æ¨¡å‹çš„åŸºæœ¬ç‰¹å¾µï¼ˆå¦‚æ¨¡å‹æ¶æ§‹ã€è¨“ç·´æŠ€è¡“ï¼‰ä»¥åŠæ¨¡å‹è¼¸å‡ºçš„å½¢å¼ã€‚",
    "ä»‹å…¥ç›®çš„ï¼šèªªæ˜äººå·¥æ™ºæ…§æ¨¡å‹è¨­è¨ˆçš„æ ¸å¿ƒç›®æ¨™ä»¥åŠé©ç”¨æƒ…å¢ƒã€‚",
    "ä»‹å…¥çš„è­¦å‘Šç¯„åœå¤–ä½¿ç”¨ï¼šèªªæ˜äººå·¥æ™ºæ…§æ¨¡å‹é©ç”¨ã€ä¸é©ç”¨ç¯„åœï¼ŒåŠå…¶å¯èƒ½ç™¼ç”Ÿä¹‹é¢¨éšªã€‚",
    "ä»‹å…¥é–‹ç™¼è©³æƒ…åŠè¼¸å…¥ç‰¹å¾µï¼šèªªæ˜äººå·¥æ™ºæ…§æ¨¡å‹æ ¸å¿ƒæŠ€è¡“ï¼ŒåŒ…å«æ•¸æ“šé›†ã€æ¨¡å‹çµæ§‹ã€è¨“ç·´æ–¹æ³•ç­‰ã€‚",
    "ç¢ºä¿ä»‹å…¥é–‹ç™¼å…¬å¹³æ€§çš„éç¨‹ï¼šèªªæ˜äººå·¥æ™ºæ…§æ¨¡å‹é–‹ç™¼éç¨‹ï¼Œæ•¸æ“šé›†å¹³è¡¡æ–¹å¼ã€‚",
    "å¤–éƒ¨é©—è­‰éç¨‹ï¼šèªªæ˜å¤–éƒ¨é©—è­‰èˆ‡è©•ä¼°éç¨‹ã€‚",
    "æ¨¡å‹è¡¨ç¾çš„é‡åŒ–æŒ‡æ¨™ï¼šèªªæ˜æ­¤äººå·¥æ™ºæ…§æ¨¡å‹çš„é‡åŒ–è©•ä¼°æŒ‡æ¨™ï¼Œå¦‚æ¨¡å‹çš„æº–ç¢ºç‡ã€æ¨¡å‹çš„å¬å›ç‡ã€æ¨¡å‹çš„F1åˆ†æ•¸ã€æ¨¡å‹çš„AUCæ›²ç·š",
    "ä»‹å…¥å¯¦æ–½å’Œä½¿ç”¨çš„æŒçºŒç¶­è­·ï¼šèªªæ˜æ¨¡å‹éƒ¨ç½²å¾Œå¦‚ä½•é€²è¡ŒæŒçºŒç¶­è­·ï¼ŒåŒ…æ‹¬æ€§èƒ½ç›£æ§ã€éŒ¯èª¤ä¿®å¾©åŠæ›´æ–°ã€‚",
    "æ›´æ–°å’ŒæŒçºŒé©—è­‰æˆ–å…¬å¹³æ€§è©•ä¼°è¨ˆåŠƒï¼šèªªæ˜å¦‚ä½•å®šæœŸé‡æ–°è¨“ç·´æ¨¡å‹ã€æ›´æ–°æ•¸æ“šé›†ï¼Œä¸¦é€²è¡ŒæŒçºŒæ€§é©—è­‰èˆ‡å…¬å¹³æ€§è©•ä¼°ï¼Œè®“æ¨¡å‹æ•ˆèƒ½ç©©å®šä¸”ç¬¦åˆå…¬å¹³æ€§æ¨™æº–ï¼Œä»¥ç¬¦åˆè‡¨åºŠéœ€æ±‚ã€‚"
]

# ---------- è¼”åŠ©å‡½å¼ ----------
def extract_text_by_line(pdf_bytes):
    """ä½¿ç”¨ PyMuPDF æŒ‰ block å–å‡ºæ–‡å­—"""
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    lines = []
    for page in doc:
        blocks = page.get_text("blocks")
        for b in blocks:
            text = b[4].strip()
            if text:
                lines.append(text)
    return "\n\n".join(lines)

def get_gemini_response(prompt):
    """ä½¿ç”¨ Gemini æ¨¡å‹å›æ‡‰ï¼ˆæ”¹ç‚ºç„¡ç‹€æ…‹çš„ generate_contentï¼‰"""
    try:
        # éŒ¯èª¤çš„ç”¨æ³•ï¼š response = chat.send_message(prompt)
        # æ­£ç¢ºçš„ç”¨æ³•ï¼š
        response = model.generate_content(prompt) 
        
        return response.text.strip()
    except Exception as e:
        # æª¢æŸ¥æ˜¯å¦æœ‰å› ç‚ºå…§å®¹éé•·æˆ–å®‰å…¨è¨­å®šè€Œè¢«é˜»æ“‹
        try:
            # å˜—è©¦è®€å–æ›´è©³ç´°çš„éŒ¯èª¤ï¼ˆå¦‚æœ API å›æ‡‰ä¸­æœ‰ï¼‰
            error_details = str(e)
            if response.prompt_feedback:
                 error_details = f"Prompt blocked: {response.prompt_feedback}"
            elif response.candidates and response.candidates[0].finish_reason != 'STOP':
                 error_details = f"Generation stopped: {response.candidates[0].finish_reason}"
            return f"âš ï¸ Gemini å‘¼å«éŒ¯èª¤ï¼š{error_details}"
        except:
             return f"âš ï¸ Gemini å‘¼å«éŒ¯èª¤ï¼š{e}"


def gen_missing_suggestion(principle_text):
    """è‹¥æ–‡ä»¶æœªæ¶µè“‹æŸé€æ˜æ€§åŸå‰‡ï¼Œè«‹ Gemini ç”Ÿæˆå»ºè­°è£œå……å…§å®¹"""
    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­ AI æ¨¡å‹é€æ˜æ€§å ±å‘Šæ’°å¯«å“¡ã€‚
ä¸‹åˆ—ç‚ºé€æ˜æ€§åŸå‰‡èªªæ˜ï¼Œè«‹å¯«å‡ºã€Œè‹¥è¦è£œä¸Šæœ¬åŸå‰‡ï¼Œä½ æœƒæ€éº¼æ’°å¯«ï¼Ÿã€ä»¥ç¬¦åˆæ¨™æº–ã€‚

é€æ˜æ€§åŸå‰‡å…§å®¹ï¼š
{principle_text}

è«‹ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œèªæ°£æ­£å¼ã€èƒ½ç›´æ¥è²¼å…¥å ±å‘Šæ–‡ä»¶ã€‚
"""
    resp = get_gemini_response(prompt)
    return resp.strip()
        
        
def build_transparency_prompts(principles, full_text, rag_docs_k=3):
    """
    ç‚ºæ¯ä¸€åŸå‰‡å»ºç«‹ promptã€‚
    """
    prompts = []
    rag_context = ""
    if vector_store:
        merged_query = " ".join(principles)
        try:
            docs = vector_store.similarity_search(merged_query, k=rag_docs_k)
            rag_context = "\n---\n".join(doc.page_content for doc in docs)
        except Exception:
            rag_context = ""

    for p in principles:
        prompt = f"""
---- è¦è«‹ä½ èªªæ˜çš„é€æ˜æ€§åŸå‰‡ ----
{p.split('ï¼š', 1)[0]}
ä½ æ˜¯ä¸€ä½ä½¿ç”¨ç¹é«”ä¸­æ–‡çš„é€æ˜æ€§åŸå‰‡è¬›è§£å“¡ï¼Œè«‹æ ¹æ“šä¸‹æ–¹ã€Œç”³è«‹æ–‡ä»¶å…§å®¹ã€åˆ¤æ–·ï¼š
1.æ˜¯å¦å­˜åœ¨ç›¸é—œæè¿°è®“ä½ å¯ä»¥ {p.split('ï¼š', 1)[1]}
2.è«‹ {p.split('ï¼š', 1)[1]}
---- æ–‡ä»¶å…§å®¹ï¼ˆç¯€éŒ„ï¼‰ ----
{full_text}
---- å‘é‡æª¢ç´¢åˆ°çš„ç›¸é—œåƒè€ƒæ®µè½ï¼ˆè‹¥æœ‰ï¼‰ ----
{rag_context}
---- å›è¦†æ ¼å¼ï¼ˆè«‹**åš´æ ¼**éµå®ˆï¼Œä»¥åˆ©ç¨‹å¼è§£æï¼‰----
ç‹€æ…‹:  å­˜åœ¨ / ä¸å­˜åœ¨
æ‘˜è¦: ï¼ˆ{p.split('ï¼š', 1)[1]}ã€‚è‹¥ä¸å­˜åœ¨ï¼Œè«‹å¯«ã€Œæœªç™¼ç¾ç›¸é—œæè¿°ã€ã€‚ï¼‰

----æ³¨æ„----
è«‹å‹¿ç›´æ¥è¤‡è£½æ–‡ä»¶ä¸­çš„ç¬¦è™Ÿæˆ–æ®µè½ï¼Œè«‹è‡ªè¡Œç”¨é€šé †ä¸­æ–‡æ‘˜è¦èªªæ˜ã€‚
"""
        prompts.append(prompt.strip())
    return prompts

def parse_transparency_response(response_text):
    response_text = response_text.strip()
    original = response_text
    status = "ç„¡æ³•åˆ¤è®€"
    summary = "æœªç™¼ç¾ç›¸é—œæè¿°"

    # --- åˆ¤æ–·ç‹€æ…‹ ---
    m = re.search(r"ç‹€æ…‹\s*[:ï¼š]\s*(å­˜åœ¨|ä¸å­˜åœ¨)", response_text)
    if m:
        status = m.group(1).strip()
    else:
        if "å­˜åœ¨" in response_text and "ä¸å­˜åœ¨" not in response_text:
            status = "å­˜åœ¨"
        elif "ä¸å­˜åœ¨" in response_text:
            status = "ä¸å­˜åœ¨"

    # --- æŠ“å–æ‘˜è¦å…§å®¹ ---
    m2 = re.search(r"æ‘˜è¦\s*[:ï¼š]\s*([\s\S]+)", response_text)
    if m2:
        summary = m2.group(1).strip()
    else:
        summary = original.replace("\n", " ").strip()

    # âœ… å¼·åˆ¶è¦å‰‡ï¼šè‹¥ç‹€æ…‹ç‚ºã€Œä¸å­˜åœ¨ã€ï¼Œæ‘˜è¦æ”¹ç‚ºã€Œæœªè¦‹ç›¸é—œæè¿°ã€
    if status == "ä¸å­˜åœ¨":
        summary = "æœªè¦‹ç›¸é—œæè¿°"

    return {"ç‹€æ…‹": status, "æ‘˜è¦": summary}

# ---------- ä¸»æµç¨‹èˆ‡ UI ----------
def main():
    st.set_page_config("ğŸ“„ AI ä»‹å…¥é€æ˜æ€§æª¢æ ¸", layout="wide")
    st.title("ğŸ“„ å–®ä¸€ PDF â€” ä¹å¤§é€æ˜æ€§åŸå‰‡è‡ªå‹•æª¢æ ¸ (Gemini)")
    st.markdown("ä¸Šå‚³å–®ä¸€ PDFï¼Œç³»çµ±æœƒé€æ¢æª¢æŸ¥ä¹å¤§é€æ˜æ€§åŸå‰‡æ˜¯å¦åœ¨æ–‡ä»¶ä¸­æ˜è¼‰ï¼Œä¸¦ç”¢ç”Ÿå¯ä¸‹è¼‰çš„ CSV æª”ã€‚")

    uploaded_pdf = st.file_uploader("ğŸ“¥ ä¸Šå‚³ IRB WORD æˆ– PDF æ–‡ä»¶ï¼ˆå–®ä¸€æª”æ¡ˆï¼‰", type=["pdf","docx"], accept_multiple_files=False)
    use_rag = st.checkbox("ğŸ” å•Ÿç”¨å‘é‡åº«ï¼ˆè‹¥å·²è¼‰å…¥ FAISSï¼Œå¯ä½¿ç”¨ RAG ä¸Šä¸‹æ–‡ï¼‰", value=True)
    analyze_btn = st.button("ğŸš€ é–‹å§‹æª¢æ ¸")

    if uploaded_pdf and analyze_btn:
        pdf_bytes = uploaded_pdf.read()
        pdf_filename = uploaded_pdf.name.rsplit(".", 1)[0]

        with st.spinner("â³ è®€å– PDF ä¸¦åˆ†æä¸­ï¼Œè«‹ç¨å€™..."):
            full_text = extract_text_by_line(pdf_bytes)
            prompts = build_transparency_prompts(
                TRANSPARENCY_PRINCIPLES, full_text,
                rag_docs_k=3 if use_rag and vector_store else 0
            )

            results = []
            for i, p in enumerate(TRANSPARENCY_PRINCIPLES):
                prompt = prompts[i]
                resp = get_gemini_response(prompt)
                parsed = parse_transparency_response(resp)

                suggestion = ""
                if parsed["æ‘˜è¦"] == "æœªè¦‹ç›¸é—œæè¿°":
                    suggestion = gen_missing_suggestion(p)

                results.append({
                    "åŸå‰‡ç·¨è™Ÿ": i+1,
                    "åŸå‰‡åç¨±": p,
                    "ç‹€æ…‹": parsed["ç‹€æ…‹"],
                    "æ‘˜è¦": parsed["æ‘˜è¦"],
                    "å»ºè­°è£œå……å…§å®¹": suggestion,
                })

        df = pd.DataFrame(results)
        df = df[["åŸå‰‡ç·¨è™Ÿ", "åŸå‰‡åç¨±", "ç‹€æ…‹", "æ‘˜è¦", "å»ºè­°è£œå……å…§å®¹"]]

        st.success("âœ… æª¢æ ¸å®Œæˆ")
        st.markdown(f"æª”æ¡ˆï¼š**{uploaded_pdf.name}**  â†’ å…±æœ‰ {len(df)} é …æª¢æ ¸çµæœ")
        st.dataframe(df, use_container_width=True)

        csv_data = df.to_csv(index=False)
        filename = f"{pdf_filename}_ä¹å¤§é€æ˜æ€§æª¢æ ¸.csv"
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è¼‰ CSVï¼š{filename}",
            data=csv_data,
            file_name=filename,
            mime="text/csv"
        )

        for idx, row in df.iterrows():
            with st.expander(f"ğŸ” ç¬¬ {row['åŸå‰‡ç·¨è™Ÿ']} é …ï¼š{row['åŸå‰‡åç¨±']} â€” ç‹€æ…‹ï¼š{row['ç‹€æ…‹']}"):
                st.markdown(f"**æ‘˜è¦**ï¼š{row['æ‘˜è¦']}")
                if row["æ‘˜è¦"] == "æœªè¦‹ç›¸é—œæè¿°":
                    st.markdown(f"**å»ºè­°è£œå……å…§å®¹**ï¼š{row['å»ºè­°è£œå……å…§å®¹']}")

    elif not uploaded_pdf:
        st.info("è«‹å…ˆä¸Šå‚³ä¸€ä»½ PDFï¼Œç„¶å¾ŒæŒ‰ã€é–‹å§‹æª¢æ ¸ã€‘ã€‚")

if __name__ == "__main__":
    main()
